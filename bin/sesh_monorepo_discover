#!/usr/bin/env ruby

require 'optparse'
require 'json'
require 'pathname'
require 'fileutils'
require 'digest'
require 'set'

# Monorepo project discovery script for sesh session management
class MonorepoDiscovery
  DEFAULT_CONFIG_FILE = File.expand_path("~/.config/sesh/monorepo.toml")
  DEFAULT_CACHE_DIR = File.expand_path("~/.cache")
  GLOBAL_CACHE_FILE = File.expand_path("~/.cache/sesh_monorepo_global.cache")
  CACHE_TTL_SEC = 3600

  def initialize
    @config = load_config
    @cache_dir = DEFAULT_CACHE_DIR
    FileUtils.mkdir_p(@cache_dir) unless Dir.exist?(@cache_dir)
  end

  def load_config(config_file = DEFAULT_CONFIG_FILE)
    config = {
      ignore_patterns: %w[
        node_modules .git .next .nuxt dist build target public coverage
        .vscode .idea __pycache__ .pytest_cache vendor tmp temp .cache logs
      ],
      project_files: %w[
        package.json project.json Dockerfile docker-compose.yml docker-compose.yaml
        Cargo.toml go.mod pyproject.toml requirements.txt pom.xml Gemfile
        composer.json Makefile CMakeLists.txt
      ],
      directory_mappings: {
        'microservices' => 'service',
        'libraries' => 'lib',
        'components' => 'comp'
      },
      max_depth: 3,
      separator: '-'
    }

    return config unless File.exist?(config_file)

    begin
      content = File.read(config_file)

      # Simple TOML parser for our specific needs
      current_section = nil
      content.each_line do |line|
        line = line.strip
        next if line.empty? || line.start_with?('#')

        if line.match(/^\[(\w+)\]$/)
          current_section = $1
        elsif line.match(/^\[(\w+)\.(\w+)\]$/)
          current_section = "#{$1}_#{$2}"
        elsif line.match(/^(\w+)\s*=\s*(.+)$/)
          key, value = $1, $2

          case current_section
          when 'ignore'
            if key == 'patterns' && value.match(/^\[(.*)\]$/)
              patterns = $1.scan(/"([^"]*)"/).flatten
              config[:ignore_patterns] = patterns unless patterns.empty?
            end
          when 'project_indicators'
            if key == 'files' && value.match(/^\[(.*)\]$/)
              files = $1.scan(/"([^"]*)"/).flatten
              config[:project_files] = files unless files.empty?
            end
          when 'directory_mappings'
            if value.match(/^"([^"]*)"$/)
              config[:directory_mappings][key] = $1
            end
          when 'naming'
            case key
            when 'max_depth'
              config[:max_depth] = value.to_i
            when 'separator'
              config[:separator] = value.gsub(/^"|"$/, '')
            end
          end
        end
      end
    rescue => e
      warn "Warning: Error parsing config file #{config_file}: #{e.message}"
    end

    config
  end

  def config_ignore_patterns
    @config_ignore_patterns ||= Set.new(@config[:ignore_patterns])
  end

  def config_project_files
    @config_project_files ||= Set.new(@config[:project_files])
  end

  def should_ignore?(dir_name)
    config_ignore_patterns.include?(dir_name)
  end

  def project_directory?(dir_path)
    directory_files = Dir.entries(dir_path).select { |entry| File.file?(File.join(dir_path, entry)) }
    directory_files = Set.new(directory_files)

    (config_project_files & directory_files).any?
  end

  def generate_session_name(full_path, root_dir)
    normalized_full_path = File.expand_path(full_path)
    normalized_root_dir = File.expand_path(root_dir)

    if normalized_full_path == normalized_root_dir
      return File.basename(normalized_root_dir).gsub(/[^a-zA-Z0-9_-]/, '_')
    end

    rel_path = Pathname.new(normalized_full_path).relative_path_from(Pathname.new(normalized_root_dir)).to_s
    path_components = rel_path.split('/')

    # If a max_depth is configured and the relative path is too deep, keep only the last N components.
    if @config[:max_depth] > 0 && path_components.length > @config[:max_depth]
      path_components = path_components.last(@config[:max_depth])
    end

    session_parts = []
    path_components.each_with_index do |component, i|
      mapped_name = @config[:directory_mappings][component] || component

      if i < path_components.length - 1
        next_component = path_components[i + 1]
        is_prefix = next_component.start_with?(component + @config[:separator])
        is_exact = next_component == component
        is_suffix = next_component.end_with?(component)

        if is_prefix || is_exact || is_suffix
          next # Skip this component if the next one already includes it
        end
      end

      session_parts << mapped_name
    end

    session_name = session_parts.join(@config[:separator])
    session_name.gsub(/[^a-zA-Z0-9_-]/, '_')
  end

  # Recursively scan directories for projects
  def scan_projects(root_dir, current_dir = nil, depth = 0)
    current_dir ||= root_dir
    projects = []

    # Limit recursion depth for performance
    return projects if depth > 10

    if project_directory?(current_dir)
      session_name = generate_session_name(current_dir, root_dir)
      projects << { path: current_dir, name: session_name, base_dir: File.basename(root_dir) }
    end

    return projects unless Dir.exist?(current_dir)

    Dir.entries(current_dir).each do |entry|
      next if entry.start_with?('.')

      subdir = File.join(current_dir, entry)
      next unless Dir.exist?(subdir)
      next if should_ignore?(entry)

      projects.concat(scan_projects(root_dir, subdir, depth + 1))
    end

    projects
  end

  def cache_file_path(root_dir = nil)
    GLOBAL_CACHE_FILE
  end

  def load_global_cache
    return {} unless File.exist?(GLOBAL_CACHE_FILE)

    cache_data = {}
    current_root = nil

    begin
      File.readlines(GLOBAL_CACHE_FILE).each do |line|
        line = line.strip
        next if line.empty?

        if line.start_with?("# ROOT: ")
          current_root = line.sub(/^# ROOT: /, "")
          cache_data[current_root] = { timestamp: nil, projects: [] }
        elsif line.start_with?("# TIMESTAMP: ") && current_root
          timestamp = line.sub(/^# TIMESTAMP: /, "").to_i
          cache_data[current_root][:timestamp] = timestamp
        elsif current_root && line.include?("|")
          parts = line.split('|')
          next unless parts.length >= 3
          cache_data[current_root][:projects] << {
            base_dir: parts[0],
            name: parts[1],
            path: parts[2]
          }
        end
      end
    rescue => e
      warn "Warning: Error reading global cache: #{e.message}"
      return {}
    end

    cache_data
  end

  def cache_valid_for_root?(root_dir)
    cache_data = load_global_cache
    return false unless cache_data[root_dir]

    timestamp = cache_data[root_dir][:timestamp]
    return false unless timestamp

    cache_age = Time.now.to_i - timestamp
    cache_age <= CACHE_TTL_SEC
  end

  def discover_projects(root_dir, use_cache: true)
    root_dir = File.expand_path(root_dir)

    if use_cache && cache_valid_for_root?(root_dir)
      cache_data = load_global_cache
      return cache_data[root_dir][:projects] if cache_data[root_dir]
    end

    projects = scan_projects(root_dir)
    update_global_cache(root_dir, projects) unless projects.empty?
    projects
  end

  def update_global_cache(root_dir, projects)
    cache_data = load_global_cache
    cache_data[root_dir] = {
      timestamp: Time.now.to_i,
      projects: projects
    }

    begin
      File.open(GLOBAL_CACHE_FILE, 'w') do |f|
        cache_data.each do |root, data|
          f.puts "# ROOT: #{root}"
          f.puts "# TIMESTAMP: #{data[:timestamp]}"
          data[:projects].each do |project|
            f.puts "#{project[:base_dir]}|#{project[:name]}|#{project[:path]}"
          end
          f.puts "" # Empty line separator
        end
      end
    rescue => e
      warn "Warning: Could not write global cache file: #{e.message}"
    end
  end

  def clear_cache
    if File.exist?(GLOBAL_CACHE_FILE)
      File.delete(GLOBAL_CACHE_FILE)
    end
    # Also clear old cache files for backward compatibility
    Dir.glob(File.join(@cache_dir, "sesh_monorepo_cache_*")).each do |file|
      File.delete(file)
    end
    puts "Cache cleared"
  end

  def get_all_cached_projects
    cache_data = load_global_cache
    all_projects = []

    cache_data.each do |root, data|
      # Only include projects from valid (non-expired) cache entries
      if cache_valid_for_root?(root)
        all_projects.concat(data[:projects])
      end
    end

    all_projects
  end

  def parse_sesh_toml(file_path)
    return [] unless File.exist?(file_path)

    sessions = []
    current_session = {}

    begin
      File.readlines(file_path).each do |line|
        line = line.strip
        next if line.empty? || line.start_with?('#')

        if line == '[[session]]'
          sessions << current_session unless current_session.empty?
          current_session = {}
        elsif line.match(/^(\w+)\s*=\s*"([^"]*)"$/)
          key, value = $1, $2
          current_session[key.to_sym] = value
        end
      end

      sessions << current_session unless current_session.empty?
    rescue => e
      warn "Warning: Error parsing sesh config file #{file_path}: #{e.message}"
    end

    # Convert to our format
    sessions.map do |session|
      result = {
        name: session[:name],
        base_dir: session[:name], # Use name as base_dir for manual sessions
      }
      result[:path] = File.expand_path(session[:path]) if session[:path]
      result[:startup_command] = session[:startup_command] if session[:startup_command]
      result
    end.compact
  end

  def merge_with_sesh_config(projects, sesh_file)
    return projects unless sesh_file && File.exist?(sesh_file)

    existing_sessions = parse_sesh_toml(sesh_file)
    existing_names = Set.new(existing_sessions.map { |s| s[:name] })

    # Filter out projects that already exist in the sesh config
    filtered_projects = projects.reject { |p| existing_names.include?(p[:name]) }

    # Combine existing sessions with new projects
    existing_sessions + filtered_projects
  end

  def output_json(projects)
    puts JSON.pretty_generate(projects.map { |p| { path: p[:path], name: p[:name], base_dir: p[:base_dir] } })
  end

  def output_sesh_config(projects)
    projects.each do |project|
      puts ""
      puts "[[session]]"
      puts "name = \"#{project[:name]}\""
      puts "path = \"#{project[:path]}\""
      if project[:startup_command]
        puts "startup_command = \"#{project[:startup_command]}\""
      else
        puts "startup_command = \"sesh_dev\""
      end
    end
  end

  def output_list(projects)
    projects.each { |project| puts "#{project[:base_dir]}|#{project[:name]}|#{project[:path]}" }
  end
end

def show_help
  command = File.basename($0)

  puts <<~HELP
    Usage: #{$0} [OPTIONS] [DIRECTORY]

    Discover projects in a monorepo and generate tmux session configurations.

    OPTIONS:
        -h, --help          Show this help message
        -f, --format FORMAT Output format: list, json, sesh (default: list)
        -c, --config FILE   Configuration file path
        -m, --merge FILE    Merge with existing sesh.toml config file
        --no-cache          Disable caching
        --clear-cache       Clear cache and exit
        --print-cache       Print cache file path and exit

    DIRECTORY:
        Root directory to scan (default: current directory)

    EXAMPLES:
        #{command}                                  # Scan current directory
        #{command} /path/to/monorepo               # Scan specific directory
        #{command} -f json                         # Output as JSON
        #{command} -f sesh >> ~/.config/sesh/sesh.toml  # Append to sesh config
        #{command} -m ~/.config/sesh/sesh_starter.toml -f sesh  # Merge with existing config
  HELP
end

def main
  options = {
    format: 'list',
    root_dir: Dir.pwd,
    use_cache: true,
    config_file: MonorepoDiscovery::DEFAULT_CONFIG_FILE,
    merge_file: nil
  }

  OptionParser.new do |opts|
    opts.on('-h', '--help', 'Show help') do
      show_help
      exit 0
    end

    opts.on('-f', '--format FORMAT', 'Output format (list, json, sesh)') do |format|
      options[:format] = format
    end

    opts.on('-c', '--config FILE', 'Configuration file path') do |config|
      options[:config_file] = config
    end

    opts.on('--no-cache', 'Disable caching') do
      options[:use_cache] = false
    end

    opts.on('--clear-cache', 'Clear cache and exit') do
      discovery = MonorepoDiscovery.new
      discovery.clear_cache
      exit 0
    end

    opts.on('--print-cache', 'Print cache file path and exit') do
      puts MonorepoDiscovery::GLOBAL_CACHE_FILE
      exit 0
    end

    opts.on('-m', '--merge FILE', 'Merge with existing sesh.toml config file') do |file|
      options[:merge_file] = file
    end
  end.parse!

  options[:root_dir] = ARGV[0] if ARGV[0]

  unless Dir.exist?(options[:root_dir])
    warn "Error: Directory not found: #{options[:root_dir]}"
    exit 1
  end

  discovery = MonorepoDiscovery.new

  if options[:merge_file]
    # When merging, get all cached projects instead of just current directory
    projects = discovery.get_all_cached_projects

    # Also scan current directory and update cache
    current_projects = discovery.discover_projects(options[:root_dir], use_cache: options[:use_cache])

    # Merge all cached projects with the sesh config
    merged_projects = discovery.merge_with_sesh_config(projects, options[:merge_file])
  else
    # Normal operation - scan current directory
    projects = discovery.discover_projects(options[:root_dir], use_cache: options[:use_cache])
    merged_projects = projects
  end

  if merged_projects.empty?
    warn "No projects found"
    exit 1
  end

  case options[:format]
  when 'json'
    discovery.output_json(merged_projects)
  when 'sesh'
    discovery.output_sesh_config(merged_projects)
  else
    discovery.output_list(merged_projects)
  end
end

main if __FILE__ == $0
